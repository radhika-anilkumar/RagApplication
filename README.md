Custom Chatbot Project Documentation
1. Project Overview
The Custom Chatbot is an end-to-end conversational AI application with:
Backend (FastAPI + LangChain + Ollama): Handles retrieval-augmented generation (RAG) using a local LLM (llama3) pulled via Ollama.


Frontend (Streamlit): Provides a user-friendly chat interface.


Vector Database (ChromaDB / FAISS): Stores and retrieves embeddings for context-aware answers.


The chatbot allows users to query documents and get intelligent, context-aware responses.
2. Tech Stack
Backend: FastAPI, LangChain, Ollama


Frontend: Streamlit


Database: ChromaDB / FAISS (for vector storage)


Embeddings: SentenceTransformers / HuggingFace embeddings


Local Model: llama3 via Ollama










3. Project Structure
custom-chatbot/
│
├── backend/
│   ├── api.py            # FastAPI endpoints
│   ├── rag_chain.py      # RAG chain builder
│   ├──  models.py
 |    ├──  prompts.py
 |    ├── utils.py
 |    └── settigs.py
│
├── frontend/
│   └── chat-ui.py        # Streamlit frontend
│
└──  requirements.txt


4. Setup Instructions
Prerequisites
Python 3.10+


Ollama installed


Git


Installation
git clone https://github.com/radhika-anilkumar/RagApplication.git
cd custom-chatbot
python -m venv .venv
.venv\Scripts\activate    # Windows
# or source .venv/bin/activate (Linux/Mac)

pip install -r requirements.txt


Pull LLM model via Ollama
ollama pull llama3

Run backend (FastAPI)
uvicorn backend.api:app --reload --port 8000

Run frontend (Streamlit)
streamlit run frontend/chat-ui.py


5. Architecture
User interacts with Streamlit chat UI.


Requests are sent to FastAPI backend.


Backend builds a RAG chain using:


Vector DB for context retrieval


Ollama llama3 model for LLM responses


Response is returned and displayed in UI.



6. Issues Faced & Solutions
Issue 1: ValidationError: Did not find huggingfacehub_api_token
Cause: Used HuggingFaceHub LLM instead of Ollama.
 Solution: Switched to OllamaLLM from langchain_community.llms.
from langchain_community.llms import Ollama
llm = Ollama(model="llama3")


Issue 2: ImportError: cannot import name 'Ollama' from 'langchain_ollama'
Cause: Wrong import.
 Solution: Use correct import:
from langchain_community.llms import Ollama



Issue 3: The answers generated by the chatbot were not based on the uploaded PDF; instead, it returned default LLM responses.
Cause: The application was not actually reading the content of the uploaded PDF file.
Solution: Install the PyPDF2 library using pip install PyPDF2, and update the code to extract text from the uploaded PDF as shown below:
from PyPDF2 import PdfReader  # at the top of the file

def extract_text_from_pdf(pdf_file):
    reader = PdfReader(pdf_file)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text

